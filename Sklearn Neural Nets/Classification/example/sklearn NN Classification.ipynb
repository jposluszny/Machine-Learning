{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Classification using Scikit-learn Neural Networks](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Patient's year of operation</th>\n",
       "      <th>Number of positive axillary nodes detected</th>\n",
       "      <th>Survival status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  Patient's year of operation  \\\n",
       "0   30                           64   \n",
       "1   30                           62   \n",
       "2   30                           65   \n",
       "3   31                           59   \n",
       "4   31                           65   \n",
       "\n",
       "   Number of positive axillary nodes detected  Survival status  \n",
       "0                                           1                1  \n",
       "1                                           3                1  \n",
       "2                                           0                1  \n",
       "3                                           2                1  \n",
       "4                                           4                1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('survival.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Survival status'], axis=1)\n",
    "y = df['Survival status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(solver='adam', max_iter=1000,verbose=True, hidden_layer_sizes=(5, 5,5), random_state=101)\n",
    "# three layers with 5  nodes each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70331645\n",
      "Iteration 2, loss = 0.70271116\n",
      "Iteration 3, loss = 0.70217755\n",
      "Iteration 4, loss = 0.70160346\n",
      "Iteration 5, loss = 0.70107763\n",
      "Iteration 6, loss = 0.70054673\n",
      "Iteration 7, loss = 0.70006284\n",
      "Iteration 8, loss = 0.69951980\n",
      "Iteration 9, loss = 0.69910452\n",
      "Iteration 10, loss = 0.69862776\n",
      "Iteration 11, loss = 0.69818549\n",
      "Iteration 12, loss = 0.69778692\n",
      "Iteration 13, loss = 0.69731693\n",
      "Iteration 14, loss = 0.69692103\n",
      "Iteration 15, loss = 0.69650596\n",
      "Iteration 16, loss = 0.69611274\n",
      "Iteration 17, loss = 0.69567738\n",
      "Iteration 18, loss = 0.69525130\n",
      "Iteration 19, loss = 0.69483272\n",
      "Iteration 20, loss = 0.69446661\n",
      "Iteration 21, loss = 0.69407659\n",
      "Iteration 22, loss = 0.69369509\n",
      "Iteration 23, loss = 0.69332325\n",
      "Iteration 24, loss = 0.69297112\n",
      "Iteration 25, loss = 0.69261800\n",
      "Iteration 26, loss = 0.69228397\n",
      "Iteration 27, loss = 0.69193265\n",
      "Iteration 28, loss = 0.69158602\n",
      "Iteration 29, loss = 0.69127469\n",
      "Iteration 30, loss = 0.69095462\n",
      "Iteration 31, loss = 0.69065162\n",
      "Iteration 32, loss = 0.69031597\n",
      "Iteration 33, loss = 0.69003389\n",
      "Iteration 34, loss = 0.68973386\n",
      "Iteration 35, loss = 0.68944220\n",
      "Iteration 36, loss = 0.68915379\n",
      "Iteration 37, loss = 0.68883827\n",
      "Iteration 38, loss = 0.68855885\n",
      "Iteration 39, loss = 0.68828354\n",
      "Iteration 40, loss = 0.68797980\n",
      "Iteration 41, loss = 0.68771491\n",
      "Iteration 42, loss = 0.68747887\n",
      "Iteration 43, loss = 0.68716258\n",
      "Iteration 44, loss = 0.68691725\n",
      "Iteration 45, loss = 0.68665358\n",
      "Iteration 46, loss = 0.68639746\n",
      "Iteration 47, loss = 0.68613931\n",
      "Iteration 48, loss = 0.68590663\n",
      "Iteration 49, loss = 0.68561535\n",
      "Iteration 50, loss = 0.68543170\n",
      "Iteration 51, loss = 0.68512485\n",
      "Iteration 52, loss = 0.68489244\n",
      "Iteration 53, loss = 0.68462070\n",
      "Iteration 54, loss = 0.68434144\n",
      "Iteration 55, loss = 0.68410301\n",
      "Iteration 56, loss = 0.68376478\n",
      "Iteration 57, loss = 0.68350239\n",
      "Iteration 58, loss = 0.68321168\n",
      "Iteration 59, loss = 0.68290736\n",
      "Iteration 60, loss = 0.68261321\n",
      "Iteration 61, loss = 0.68226243\n",
      "Iteration 62, loss = 0.68192597\n",
      "Iteration 63, loss = 0.68159239\n",
      "Iteration 64, loss = 0.68124708\n",
      "Iteration 65, loss = 0.68084958\n",
      "Iteration 66, loss = 0.68050152\n",
      "Iteration 67, loss = 0.68011334\n",
      "Iteration 68, loss = 0.67971526\n",
      "Iteration 69, loss = 0.67928237\n",
      "Iteration 70, loss = 0.67890264\n",
      "Iteration 71, loss = 0.67844841\n",
      "Iteration 72, loss = 0.67805448\n",
      "Iteration 73, loss = 0.67763492\n",
      "Iteration 74, loss = 0.67717875\n",
      "Iteration 75, loss = 0.67666390\n",
      "Iteration 76, loss = 0.67626406\n",
      "Iteration 77, loss = 0.67574245\n",
      "Iteration 78, loss = 0.67525046\n",
      "Iteration 79, loss = 0.67472755\n",
      "Iteration 80, loss = 0.67424435\n",
      "Iteration 81, loss = 0.67370846\n",
      "Iteration 82, loss = 0.67310263\n",
      "Iteration 83, loss = 0.67260331\n",
      "Iteration 84, loss = 0.67209497\n",
      "Iteration 85, loss = 0.67145915\n",
      "Iteration 86, loss = 0.67089738\n",
      "Iteration 87, loss = 0.67031769\n",
      "Iteration 88, loss = 0.66972471\n",
      "Iteration 89, loss = 0.66908009\n",
      "Iteration 90, loss = 0.66848680\n",
      "Iteration 91, loss = 0.66791632\n",
      "Iteration 92, loss = 0.66722744\n",
      "Iteration 93, loss = 0.66663388\n",
      "Iteration 94, loss = 0.66598359\n",
      "Iteration 95, loss = 0.66535408\n",
      "Iteration 96, loss = 0.66468695\n",
      "Iteration 97, loss = 0.66406991\n",
      "Iteration 98, loss = 0.66341476\n",
      "Iteration 99, loss = 0.66273625\n",
      "Iteration 100, loss = 0.66204280\n",
      "Iteration 101, loss = 0.66134943\n",
      "Iteration 102, loss = 0.66060651\n",
      "Iteration 103, loss = 0.65988778\n",
      "Iteration 104, loss = 0.65910912\n",
      "Iteration 105, loss = 0.65820548\n",
      "Iteration 106, loss = 0.65730591\n",
      "Iteration 107, loss = 0.65646435\n",
      "Iteration 108, loss = 0.65561715\n",
      "Iteration 109, loss = 0.65467879\n",
      "Iteration 110, loss = 0.65382176\n",
      "Iteration 111, loss = 0.65296172\n",
      "Iteration 112, loss = 0.65215782\n",
      "Iteration 113, loss = 0.65122757\n",
      "Iteration 114, loss = 0.65042963\n",
      "Iteration 115, loss = 0.64958327\n",
      "Iteration 116, loss = 0.64881555\n",
      "Iteration 117, loss = 0.64802831\n",
      "Iteration 118, loss = 0.64712150\n",
      "Iteration 119, loss = 0.64626943\n",
      "Iteration 120, loss = 0.64540121\n",
      "Iteration 121, loss = 0.64454768\n",
      "Iteration 122, loss = 0.64363139\n",
      "Iteration 123, loss = 0.64271083\n",
      "Iteration 124, loss = 0.64178833\n",
      "Iteration 125, loss = 0.64090761\n",
      "Iteration 126, loss = 0.64003868\n",
      "Iteration 127, loss = 0.63905523\n",
      "Iteration 128, loss = 0.63822915\n",
      "Iteration 129, loss = 0.63729699\n",
      "Iteration 130, loss = 0.63639864\n",
      "Iteration 131, loss = 0.63548556\n",
      "Iteration 132, loss = 0.63458125\n",
      "Iteration 133, loss = 0.63362792\n",
      "Iteration 134, loss = 0.63275780\n",
      "Iteration 135, loss = 0.63185814\n",
      "Iteration 136, loss = 0.63085920\n",
      "Iteration 137, loss = 0.62929449\n",
      "Iteration 138, loss = 0.62748307\n",
      "Iteration 139, loss = 0.62550507\n",
      "Iteration 140, loss = 0.62316083\n",
      "Iteration 141, loss = 0.62098781\n",
      "Iteration 142, loss = 0.61825616\n",
      "Iteration 143, loss = 0.61527543\n",
      "Iteration 144, loss = 0.61196827\n",
      "Iteration 145, loss = 0.60867787\n",
      "Iteration 146, loss = 0.60531606\n",
      "Iteration 147, loss = 0.60162321\n",
      "Iteration 148, loss = 0.59828122\n",
      "Iteration 149, loss = 0.59459824\n",
      "Iteration 150, loss = 0.59127049\n",
      "Iteration 151, loss = 0.58795701\n",
      "Iteration 152, loss = 0.58435602\n",
      "Iteration 153, loss = 0.58080975\n",
      "Iteration 154, loss = 0.57743168\n",
      "Iteration 155, loss = 0.57404782\n",
      "Iteration 156, loss = 0.57070688\n",
      "Iteration 157, loss = 0.56743570\n",
      "Iteration 158, loss = 0.56433863\n",
      "Iteration 159, loss = 0.56095342\n",
      "Iteration 160, loss = 0.55770876\n",
      "Iteration 161, loss = 0.55452911\n",
      "Iteration 162, loss = 0.55160944\n",
      "Iteration 163, loss = 0.54868226\n",
      "Iteration 164, loss = 0.54578514\n",
      "Iteration 165, loss = 0.54307660\n",
      "Iteration 166, loss = 0.54042363\n",
      "Iteration 167, loss = 0.53812572\n",
      "Iteration 168, loss = 0.53548879\n",
      "Iteration 169, loss = 0.53337577\n",
      "Iteration 170, loss = 0.53115203\n",
      "Iteration 171, loss = 0.52918521\n",
      "Iteration 172, loss = 0.52730279\n",
      "Iteration 173, loss = 0.52545007\n",
      "Iteration 174, loss = 0.52376180\n",
      "Iteration 175, loss = 0.52214659\n",
      "Iteration 176, loss = 0.52058946\n",
      "Iteration 177, loss = 0.51910883\n",
      "Iteration 178, loss = 0.51768398\n",
      "Iteration 179, loss = 0.51634211\n",
      "Iteration 180, loss = 0.51500169\n",
      "Iteration 181, loss = 0.51388150\n",
      "Iteration 182, loss = 0.51265910\n",
      "Iteration 183, loss = 0.51147550\n",
      "Iteration 184, loss = 0.51045737\n",
      "Iteration 185, loss = 0.50936604\n",
      "Iteration 186, loss = 0.50838880\n",
      "Iteration 187, loss = 0.50742793\n",
      "Iteration 188, loss = 0.50648955\n",
      "Iteration 189, loss = 0.50566964\n",
      "Iteration 190, loss = 0.50476873\n",
      "Iteration 191, loss = 0.50398446\n",
      "Iteration 192, loss = 0.50315085\n",
      "Iteration 193, loss = 0.50243545\n",
      "Iteration 194, loss = 0.50166246\n",
      "Iteration 195, loss = 0.50097399\n",
      "Iteration 196, loss = 0.50028564\n",
      "Iteration 197, loss = 0.49967506\n",
      "Iteration 198, loss = 0.49902646\n",
      "Iteration 199, loss = 0.49838536\n",
      "Iteration 200, loss = 0.49772742\n",
      "Iteration 201, loss = 0.49711488\n",
      "Iteration 202, loss = 0.49652972\n",
      "Iteration 203, loss = 0.49587060\n",
      "Iteration 204, loss = 0.49528918\n",
      "Iteration 205, loss = 0.49465894\n",
      "Iteration 206, loss = 0.49409915\n",
      "Iteration 207, loss = 0.49355825\n",
      "Iteration 208, loss = 0.49300679\n",
      "Iteration 209, loss = 0.49246273\n",
      "Iteration 210, loss = 0.49192802\n",
      "Iteration 211, loss = 0.49144255\n",
      "Iteration 212, loss = 0.49100693\n",
      "Iteration 213, loss = 0.49046618\n",
      "Iteration 214, loss = 0.49001352\n",
      "Iteration 215, loss = 0.48962305\n",
      "Iteration 216, loss = 0.48916341\n",
      "Iteration 217, loss = 0.48870664\n",
      "Iteration 218, loss = 0.48835467\n",
      "Iteration 219, loss = 0.48796295\n",
      "Iteration 220, loss = 0.48755064\n",
      "Iteration 221, loss = 0.48716877\n",
      "Iteration 222, loss = 0.48680536\n",
      "Iteration 223, loss = 0.48646671\n",
      "Iteration 224, loss = 0.48609754\n",
      "Iteration 225, loss = 0.48579744\n",
      "Iteration 226, loss = 0.48538789\n",
      "Iteration 227, loss = 0.48508108\n",
      "Iteration 228, loss = 0.48480844\n",
      "Iteration 229, loss = 0.48445496\n",
      "Iteration 230, loss = 0.48414967\n",
      "Iteration 231, loss = 0.48383142\n",
      "Iteration 232, loss = 0.48354038\n",
      "Iteration 233, loss = 0.48322223\n",
      "Iteration 234, loss = 0.48293151\n",
      "Iteration 235, loss = 0.48265431\n",
      "Iteration 236, loss = 0.48234771\n",
      "Iteration 237, loss = 0.48211842\n",
      "Iteration 238, loss = 0.48182933\n",
      "Iteration 239, loss = 0.48155699\n",
      "Iteration 240, loss = 0.48131434\n",
      "Iteration 241, loss = 0.48100156\n",
      "Iteration 242, loss = 0.48071894\n",
      "Iteration 243, loss = 0.48045515\n",
      "Iteration 244, loss = 0.48023051\n",
      "Iteration 245, loss = 0.47998264\n",
      "Iteration 246, loss = 0.47974616\n",
      "Iteration 247, loss = 0.47953734\n",
      "Iteration 248, loss = 0.47932275\n",
      "Iteration 249, loss = 0.47908114\n",
      "Iteration 250, loss = 0.47888394\n",
      "Iteration 251, loss = 0.47863708\n",
      "Iteration 252, loss = 0.47843963\n",
      "Iteration 253, loss = 0.47823360\n",
      "Iteration 254, loss = 0.47806469\n",
      "Iteration 255, loss = 0.47783499\n",
      "Iteration 256, loss = 0.47760771\n",
      "Iteration 257, loss = 0.47738215\n",
      "Iteration 258, loss = 0.47720005\n",
      "Iteration 259, loss = 0.47700052\n",
      "Iteration 260, loss = 0.47675540\n",
      "Iteration 261, loss = 0.47661296\n",
      "Iteration 262, loss = 0.47640909\n",
      "Iteration 263, loss = 0.47623120\n",
      "Iteration 264, loss = 0.47601672\n",
      "Iteration 265, loss = 0.47592002\n",
      "Iteration 266, loss = 0.47568144\n",
      "Iteration 267, loss = 0.47550103\n",
      "Iteration 268, loss = 0.47536489\n",
      "Iteration 269, loss = 0.47514811\n",
      "Iteration 270, loss = 0.47497357\n",
      "Iteration 271, loss = 0.47483381\n",
      "Iteration 272, loss = 0.47463493\n",
      "Iteration 273, loss = 0.47445754\n",
      "Iteration 274, loss = 0.47426043\n",
      "Iteration 275, loss = 0.47403613\n",
      "Iteration 276, loss = 0.47385682\n",
      "Iteration 277, loss = 0.47371986\n",
      "Iteration 278, loss = 0.47349749\n",
      "Iteration 279, loss = 0.47332084\n",
      "Iteration 280, loss = 0.47312952\n",
      "Iteration 281, loss = 0.47294914\n",
      "Iteration 282, loss = 0.47278735\n",
      "Iteration 283, loss = 0.47263316\n",
      "Iteration 284, loss = 0.47243474\n",
      "Iteration 285, loss = 0.47225455\n",
      "Iteration 286, loss = 0.47214432\n",
      "Iteration 287, loss = 0.47195019\n",
      "Iteration 288, loss = 0.47176486\n",
      "Iteration 289, loss = 0.47156730\n",
      "Iteration 290, loss = 0.47139368\n",
      "Iteration 291, loss = 0.47121366\n",
      "Iteration 292, loss = 0.47105170\n",
      "Iteration 293, loss = 0.47091575\n",
      "Iteration 294, loss = 0.47073907\n",
      "Iteration 295, loss = 0.47058416\n",
      "Iteration 296, loss = 0.47042378\n",
      "Iteration 297, loss = 0.47027921\n",
      "Iteration 298, loss = 0.47015066\n",
      "Iteration 299, loss = 0.47006536\n",
      "Iteration 300, loss = 0.46990440\n",
      "Iteration 301, loss = 0.46975092\n",
      "Iteration 302, loss = 0.46960179\n",
      "Iteration 303, loss = 0.46947413\n",
      "Iteration 304, loss = 0.46933699\n",
      "Iteration 305, loss = 0.46919629\n",
      "Iteration 306, loss = 0.46904859\n",
      "Iteration 307, loss = 0.46892923\n",
      "Iteration 308, loss = 0.46879234\n",
      "Iteration 309, loss = 0.46866885\n",
      "Iteration 310, loss = 0.46855756\n",
      "Iteration 311, loss = 0.46841279\n",
      "Iteration 312, loss = 0.46824781\n",
      "Iteration 313, loss = 0.46806379\n",
      "Iteration 314, loss = 0.46786954\n",
      "Iteration 315, loss = 0.46765972\n",
      "Iteration 316, loss = 0.46744710\n",
      "Iteration 317, loss = 0.46729422\n",
      "Iteration 318, loss = 0.46703947\n",
      "Iteration 319, loss = 0.46685691\n",
      "Iteration 320, loss = 0.46665519\n",
      "Iteration 321, loss = 0.46647072\n",
      "Iteration 322, loss = 0.46628056\n",
      "Iteration 323, loss = 0.46608389\n",
      "Iteration 324, loss = 0.46590653\n",
      "Iteration 325, loss = 0.46573180\n",
      "Iteration 326, loss = 0.46565218\n",
      "Iteration 327, loss = 0.46550671\n",
      "Iteration 328, loss = 0.46533698\n",
      "Iteration 329, loss = 0.46512527\n",
      "Iteration 330, loss = 0.46492392\n",
      "Iteration 331, loss = 0.46474065\n",
      "Iteration 332, loss = 0.46457768\n",
      "Iteration 333, loss = 0.46441722\n",
      "Iteration 334, loss = 0.46423702\n",
      "Iteration 335, loss = 0.46412696\n",
      "Iteration 336, loss = 0.46398287\n",
      "Iteration 337, loss = 0.46373229\n",
      "Iteration 338, loss = 0.46362676\n",
      "Iteration 339, loss = 0.46342864\n",
      "Iteration 340, loss = 0.46327890\n",
      "Iteration 341, loss = 0.46305127\n",
      "Iteration 342, loss = 0.46289524\n",
      "Iteration 343, loss = 0.46267602\n",
      "Iteration 344, loss = 0.46248840\n",
      "Iteration 345, loss = 0.46233731\n",
      "Iteration 346, loss = 0.46214238\n",
      "Iteration 347, loss = 0.46197025\n",
      "Iteration 348, loss = 0.46178257\n",
      "Iteration 349, loss = 0.46162575\n",
      "Iteration 350, loss = 0.46148532\n",
      "Iteration 351, loss = 0.46139131\n",
      "Iteration 352, loss = 0.46123735\n",
      "Iteration 353, loss = 0.46111448\n",
      "Iteration 354, loss = 0.46103589\n",
      "Iteration 355, loss = 0.46082374\n",
      "Iteration 356, loss = 0.46066643\n",
      "Iteration 357, loss = 0.46044489\n",
      "Iteration 358, loss = 0.46024516\n",
      "Iteration 359, loss = 0.46009891\n",
      "Iteration 360, loss = 0.45989492\n",
      "Iteration 361, loss = 0.45970294\n",
      "Iteration 362, loss = 0.45954927\n",
      "Iteration 363, loss = 0.45941328\n",
      "Iteration 364, loss = 0.45924436\n",
      "Iteration 365, loss = 0.45908307\n",
      "Iteration 366, loss = 0.45890538\n",
      "Iteration 367, loss = 0.45872818\n",
      "Iteration 368, loss = 0.45854213\n",
      "Iteration 369, loss = 0.45839941\n",
      "Iteration 370, loss = 0.45822787\n",
      "Iteration 371, loss = 0.45805862\n",
      "Iteration 372, loss = 0.45786274\n",
      "Iteration 373, loss = 0.45771222\n",
      "Iteration 374, loss = 0.45754424\n",
      "Iteration 375, loss = 0.45738557\n",
      "Iteration 376, loss = 0.45720220\n",
      "Iteration 377, loss = 0.45703382\n",
      "Iteration 378, loss = 0.45691334\n",
      "Iteration 379, loss = 0.45675276\n",
      "Iteration 380, loss = 0.45660324\n",
      "Iteration 381, loss = 0.45645140\n",
      "Iteration 382, loss = 0.45628851\n",
      "Iteration 383, loss = 0.45612057\n",
      "Iteration 384, loss = 0.45595743\n",
      "Iteration 385, loss = 0.45585155\n",
      "Iteration 386, loss = 0.45571189\n",
      "Iteration 387, loss = 0.45568994\n",
      "Iteration 388, loss = 0.45549034\n",
      "Iteration 389, loss = 0.45536949\n",
      "Iteration 390, loss = 0.45525572\n",
      "Iteration 391, loss = 0.45512969\n",
      "Iteration 392, loss = 0.45499155\n",
      "Iteration 393, loss = 0.45488482\n",
      "Iteration 394, loss = 0.45475451\n",
      "Iteration 395, loss = 0.45461491\n",
      "Iteration 396, loss = 0.45448586\n",
      "Iteration 397, loss = 0.45439035\n",
      "Iteration 398, loss = 0.45427764\n",
      "Iteration 399, loss = 0.45418511\n",
      "Iteration 400, loss = 0.45414037\n",
      "Iteration 401, loss = 0.45399663\n",
      "Iteration 402, loss = 0.45387056\n",
      "Iteration 403, loss = 0.45373298\n",
      "Iteration 404, loss = 0.45363616\n",
      "Iteration 405, loss = 0.45354217\n",
      "Iteration 406, loss = 0.45344186\n",
      "Iteration 407, loss = 0.45334153\n",
      "Iteration 408, loss = 0.45322092\n",
      "Iteration 409, loss = 0.45310893\n",
      "Iteration 410, loss = 0.45303129\n",
      "Iteration 411, loss = 0.45290863\n",
      "Iteration 412, loss = 0.45282505\n",
      "Iteration 413, loss = 0.45268644\n",
      "Iteration 414, loss = 0.45271913\n",
      "Iteration 415, loss = 0.45257550\n",
      "Iteration 416, loss = 0.45242986\n",
      "Iteration 417, loss = 0.45228142\n",
      "Iteration 418, loss = 0.45216321\n",
      "Iteration 419, loss = 0.45200434\n",
      "Iteration 420, loss = 0.45186840\n",
      "Iteration 421, loss = 0.45177523\n",
      "Iteration 422, loss = 0.45161597\n",
      "Iteration 423, loss = 0.45149749\n",
      "Iteration 424, loss = 0.45154129\n",
      "Iteration 425, loss = 0.45144077\n",
      "Iteration 426, loss = 0.45140789\n",
      "Iteration 427, loss = 0.45135155\n",
      "Iteration 428, loss = 0.45124637\n",
      "Iteration 429, loss = 0.45107749\n",
      "Iteration 430, loss = 0.45087034\n",
      "Iteration 431, loss = 0.45064911\n",
      "Iteration 432, loss = 0.45049405\n",
      "Iteration 433, loss = 0.45043937\n",
      "Iteration 434, loss = 0.45038730\n",
      "Iteration 435, loss = 0.45034644\n",
      "Iteration 436, loss = 0.45035090\n",
      "Iteration 437, loss = 0.45043223\n",
      "Iteration 438, loss = 0.45040249\n",
      "Iteration 439, loss = 0.45039728\n",
      "Iteration 440, loss = 0.45025586\n",
      "Iteration 441, loss = 0.45007072\n",
      "Iteration 442, loss = 0.44994954\n",
      "Iteration 443, loss = 0.44980264\n",
      "Iteration 444, loss = 0.44969166\n",
      "Iteration 445, loss = 0.44957216\n",
      "Iteration 446, loss = 0.44950234\n",
      "Iteration 447, loss = 0.44936793\n",
      "Iteration 448, loss = 0.44923170\n",
      "Iteration 449, loss = 0.44915409\n",
      "Iteration 450, loss = 0.44893770\n",
      "Iteration 451, loss = 0.44881718\n",
      "Iteration 452, loss = 0.44867999\n",
      "Iteration 453, loss = 0.44857971\n",
      "Iteration 454, loss = 0.44846782\n",
      "Iteration 455, loss = 0.44835034\n",
      "Iteration 456, loss = 0.44826438\n",
      "Iteration 457, loss = 0.44813345\n",
      "Iteration 458, loss = 0.44805697\n",
      "Iteration 459, loss = 0.44794715\n",
      "Iteration 460, loss = 0.44785874\n",
      "Iteration 461, loss = 0.44773897\n",
      "Iteration 462, loss = 0.44765947\n",
      "Iteration 463, loss = 0.44751982\n",
      "Iteration 464, loss = 0.44741541\n",
      "Iteration 465, loss = 0.44733256\n",
      "Iteration 466, loss = 0.44722904\n",
      "Iteration 467, loss = 0.44711067\n",
      "Iteration 468, loss = 0.44702129\n",
      "Iteration 469, loss = 0.44693232\n",
      "Iteration 470, loss = 0.44684544\n",
      "Iteration 471, loss = 0.44671893\n",
      "Iteration 472, loss = 0.44664146\n",
      "Iteration 473, loss = 0.44654442\n",
      "Iteration 474, loss = 0.44643064\n",
      "Iteration 475, loss = 0.44630847\n",
      "Iteration 476, loss = 0.44623160\n",
      "Iteration 477, loss = 0.44614609\n",
      "Iteration 478, loss = 0.44601132\n",
      "Iteration 479, loss = 0.44593289\n",
      "Iteration 480, loss = 0.44582481\n",
      "Iteration 481, loss = 0.44574282\n",
      "Iteration 482, loss = 0.44563185\n",
      "Iteration 483, loss = 0.44557085\n",
      "Iteration 484, loss = 0.44544533\n",
      "Iteration 485, loss = 0.44538474\n",
      "Iteration 486, loss = 0.44531167\n",
      "Iteration 487, loss = 0.44526699\n",
      "Iteration 488, loss = 0.44515610\n",
      "Iteration 489, loss = 0.44509096\n",
      "Iteration 490, loss = 0.44495938\n",
      "Iteration 491, loss = 0.44486552\n",
      "Iteration 492, loss = 0.44478324\n",
      "Iteration 493, loss = 0.44463361\n",
      "Iteration 494, loss = 0.44456348\n",
      "Iteration 495, loss = 0.44449120\n",
      "Iteration 496, loss = 0.44441359\n",
      "Iteration 497, loss = 0.44431669\n",
      "Iteration 498, loss = 0.44424273\n",
      "Iteration 499, loss = 0.44412402\n",
      "Iteration 500, loss = 0.44403991\n",
      "Iteration 501, loss = 0.44396846\n",
      "Iteration 502, loss = 0.44387825\n",
      "Iteration 503, loss = 0.44381889\n",
      "Iteration 504, loss = 0.44376577\n",
      "Iteration 505, loss = 0.44366257\n",
      "Iteration 506, loss = 0.44358696\n",
      "Iteration 507, loss = 0.44352738\n",
      "Iteration 508, loss = 0.44348759\n",
      "Iteration 509, loss = 0.44338873\n",
      "Iteration 510, loss = 0.44331153\n",
      "Iteration 511, loss = 0.44311180\n",
      "Iteration 512, loss = 0.44299280\n",
      "Iteration 513, loss = 0.44288925\n",
      "Iteration 514, loss = 0.44274783\n",
      "Iteration 515, loss = 0.44258610\n",
      "Iteration 516, loss = 0.44241894\n",
      "Iteration 517, loss = 0.44223103\n",
      "Iteration 518, loss = 0.44205323\n",
      "Iteration 519, loss = 0.44188829\n",
      "Iteration 520, loss = 0.44180392\n",
      "Iteration 521, loss = 0.44166630\n",
      "Iteration 522, loss = 0.44159588\n",
      "Iteration 523, loss = 0.44147032\n",
      "Iteration 524, loss = 0.44137918\n",
      "Iteration 525, loss = 0.44126596\n",
      "Iteration 526, loss = 0.44117907\n",
      "Iteration 527, loss = 0.44108969\n",
      "Iteration 528, loss = 0.44101361\n",
      "Iteration 529, loss = 0.44094467\n",
      "Iteration 530, loss = 0.44089465\n",
      "Iteration 531, loss = 0.44077751\n",
      "Iteration 532, loss = 0.44065423\n",
      "Iteration 533, loss = 0.44055645\n",
      "Iteration 534, loss = 0.44047297\n",
      "Iteration 535, loss = 0.44032855\n",
      "Iteration 536, loss = 0.44022124\n",
      "Iteration 537, loss = 0.44013187\n",
      "Iteration 538, loss = 0.44002569\n",
      "Iteration 539, loss = 0.43989721\n",
      "Iteration 540, loss = 0.43981852\n",
      "Iteration 541, loss = 0.43970393\n",
      "Iteration 542, loss = 0.43957450\n",
      "Iteration 543, loss = 0.43946424\n",
      "Iteration 544, loss = 0.43938575\n",
      "Iteration 545, loss = 0.43926316\n",
      "Iteration 546, loss = 0.43912791\n",
      "Iteration 547, loss = 0.43903513\n",
      "Iteration 548, loss = 0.43898169\n",
      "Iteration 549, loss = 0.43887954\n",
      "Iteration 550, loss = 0.43875607\n",
      "Iteration 551, loss = 0.43863825\n",
      "Iteration 552, loss = 0.43852333\n",
      "Iteration 553, loss = 0.43843874\n",
      "Iteration 554, loss = 0.43834480\n",
      "Iteration 555, loss = 0.43824497\n",
      "Iteration 556, loss = 0.43817627\n",
      "Iteration 557, loss = 0.43806722\n",
      "Iteration 558, loss = 0.43797282\n",
      "Iteration 559, loss = 0.43787162\n",
      "Iteration 560, loss = 0.43778072\n",
      "Iteration 561, loss = 0.43767642\n",
      "Iteration 562, loss = 0.43758653\n",
      "Iteration 563, loss = 0.43751810\n",
      "Iteration 564, loss = 0.43751670\n",
      "Iteration 565, loss = 0.43739537\n",
      "Iteration 566, loss = 0.43726615\n",
      "Iteration 567, loss = 0.43716259\n",
      "Iteration 568, loss = 0.43706323\n",
      "Iteration 569, loss = 0.43693844\n",
      "Iteration 570, loss = 0.43686111\n",
      "Iteration 571, loss = 0.43677391\n",
      "Iteration 572, loss = 0.43673363\n",
      "Iteration 573, loss = 0.43665680\n",
      "Iteration 574, loss = 0.43655116\n",
      "Iteration 575, loss = 0.43648515\n",
      "Iteration 576, loss = 0.43640448\n",
      "Iteration 577, loss = 0.43634303\n",
      "Iteration 578, loss = 0.43624929\n",
      "Iteration 579, loss = 0.43614491\n",
      "Iteration 580, loss = 0.43604596\n",
      "Iteration 581, loss = 0.43595326\n",
      "Iteration 582, loss = 0.43584889\n",
      "Iteration 583, loss = 0.43575462\n",
      "Iteration 584, loss = 0.43576308\n",
      "Iteration 585, loss = 0.43556777\n",
      "Iteration 586, loss = 0.43547729\n",
      "Iteration 587, loss = 0.43541021\n",
      "Iteration 588, loss = 0.43536530\n",
      "Iteration 589, loss = 0.43523537\n",
      "Iteration 590, loss = 0.43512699\n",
      "Iteration 591, loss = 0.43505890\n",
      "Iteration 592, loss = 0.43499834\n",
      "Iteration 593, loss = 0.43494659\n",
      "Iteration 594, loss = 0.43480947\n",
      "Iteration 595, loss = 0.43469379\n",
      "Iteration 596, loss = 0.43459102\n",
      "Iteration 597, loss = 0.43446065\n",
      "Iteration 598, loss = 0.43459220\n",
      "Iteration 599, loss = 0.43441942\n",
      "Iteration 600, loss = 0.43432938\n",
      "Iteration 601, loss = 0.43423467\n",
      "Iteration 602, loss = 0.43416084\n",
      "Iteration 603, loss = 0.43403846\n",
      "Iteration 604, loss = 0.43393546\n",
      "Iteration 605, loss = 0.43391144\n",
      "Iteration 606, loss = 0.43375131\n",
      "Iteration 607, loss = 0.43371471\n",
      "Iteration 608, loss = 0.43364031\n",
      "Iteration 609, loss = 0.43359547\n",
      "Iteration 610, loss = 0.43343355\n",
      "Iteration 611, loss = 0.43337921\n",
      "Iteration 612, loss = 0.43331999\n",
      "Iteration 613, loss = 0.43321624\n",
      "Iteration 614, loss = 0.43314949\n",
      "Iteration 615, loss = 0.43306922\n",
      "Iteration 616, loss = 0.43297267\n",
      "Iteration 617, loss = 0.43286949\n",
      "Iteration 618, loss = 0.43280080\n",
      "Iteration 619, loss = 0.43273130\n",
      "Iteration 620, loss = 0.43266842\n",
      "Iteration 621, loss = 0.43259916\n",
      "Iteration 622, loss = 0.43265112\n",
      "Iteration 623, loss = 0.43252629\n",
      "Iteration 624, loss = 0.43248717\n",
      "Iteration 625, loss = 0.43239307\n",
      "Iteration 626, loss = 0.43230698\n",
      "Iteration 627, loss = 0.43226759\n",
      "Iteration 628, loss = 0.43220061\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(5, 5, 5), max_iter=1000, random_state=101,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8198198198198198"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[65, 14],\n",
       "       [ 6, 26]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.82      0.87        79\n",
      "           2       0.65      0.81      0.72        32\n",
      "\n",
      "    accuracy                           0.82       111\n",
      "   macro avg       0.78      0.82      0.79       111\n",
      "weighted avg       0.84      0.82      0.83       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
